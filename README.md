# SmartChat - Assistente Virtual com RAG

![Java](https://img.shields.io/badge/Java-17%2B-blue)
![Spring Boot](https://img.shields.io/badge/Spring_Boot-3.1-green)
![Docker](https://img.shields.io/badge/Docker-‚úì-blue)

## üìù Vis√£o Geral

Este projeto implementa a t√©cnica **RAG (Retrieval Augmented Generation)** para responder perguntas com base em contexto armazenado em um mecanismo de busca vetorial (Azure Search). Utiliza-se o modelo GPT da OpenAI para gera√ß√£o de respostas mais ricas e personalizadas.

### Tecnologias Principais:

- Java + Spring Boot
- Spring Cloud OpenFeign
- OpenAI Embeddings + GPT
- Banco de dados vetorial Azure AI Search
- Banco de dados relacional H2
- Docker

---

## üöÄ Come√ßando

### Pr√©-requisitos

- Docker
- Java 17+ (caso queira rodar sem Docker)
- Chave de API da OpenAI
- Token da Azure Cognitive Search com √≠ndice previamente configurado e populado

### ‚öôÔ∏è Executando o Projeto

### 1. Clone o reposit√≥rio:
```
git clone https://github.com/fabiobastian/SmartChat.git
cd SmartChat
```

### 2. Fa√ßa o build da imagem usando docker
```
docker build -t smartchat . 
```

### 3. Configure as vari√°veis de ambiente:

Crie um arquivo **.env** na raiz do projeto com o seguinte conte√∫do:
```
OPENAI_API_KEY=<sua_chave_aqui>
AZURE_SEARCH_API_KEY=<sua_chave_aqui>
```
A cria√ß√£o do .env √© opcional, mas facilita a execu√ß√£o do cont√™iner.

### 4. Execute o cont√™iner com Docker

Com **.env**
```
docker run -d --env-file .env -p 8080:8080 smartchat
```
Sem **.env**
```
docker run -d -p 8080:8080 -e "OPENAI_API_KEY=<sua_chave_aqui>" -e "AZURE_SEARCH_API_KEY=<sua_chave_aqui>" smartchat
```

## üåê Acessos √öteis

### Swagger

http://localhost:8080/swagger-ui/index.html

Interface para testar facilmente os endpoints da API.

### Console H2

http://localhost:8080/h2-console

Banco relacional em mem√≥ria (estilo SQLite) embutido no Spring Boot. S√≥ estar√° acess√≠vel enquanto a aplica√ß√£o estiver em execu√ß√£o.

Configura√ß√µes de acesso:

- Driver Class:	org.h2.Driver
- JDBC URL: jdbc:h2:file:/app/data/h2db
- User Name: sa
- Password: (deixe em branco)

Se estiver rodando fora do Docker, altere o **JDBC URL** para:

jdbc:h2:file:./data/h2db

As configura√ß√µes podem ser personalizadas no arquivo application.properties.

## üß† Principais Decis√µes T√©cnicas

### Java + Spring Boot

- Produtividade com robustez: Usei Java com Spring Boot por oferecer rapidez no desenvolvimento de APIs RESTful com qualidade de produ√ß√£o. A estrutura do framework favorece a organiza√ß√£o do projeto em camadas bem definidas.

- Feign Client para comunica√ß√£o externa: Escolhi OpenFeign por permitir chamadas HTTP limpas e tipadas, facilitando a integra√ß√£o com a API da OpenAI com pouco c√≥digo e alta legibilidade.

- F√°cil integra√ß√£o com banco de dados, e op√ß√µes de banco de dados como o H2 que se integra muito bem com o JPA, mantendo a modularidade, podendo ser migrado facilmente para bancos de dados como PostgreSQL ou Oracle.

### Embeddings

- Problema: Recebo diversas mensagens, como as devo processar?

Para resolver este problema adotei a estrat√©gia de, unir todas as mensagens vindas por meio da rule=user, e remover as sauda√ß√µes do usu√°rio, como Hello, Morning, etc. Retiro as sauda√ß√µes por elas n√£o trazerem um valor sem√¢ntico a frase, com est√° grande frase fa√ßo o embedding e posteriormente pesquiso as perguntas+respostas correspondentes no Azure.

Posteriormente com todas as informa√ß√µes em m√£os, fa√ßo o prompt para enviar para o ChatGPT, por√©m n√£o envio a mensagem com as sauda√ß√µes retiradas, envio a mensagem original do usu√°rio, para que assim a IA consiga responder da forma mais natural poss√≠vel, saudando o usu√°rio de volta.

### Escalar para N2

- Problema: Em qual momento, e qual estrat√©gia devo utilizar para escalar o problema para N2?

Adotei a estrat√©gia de **Option 2: Handover Feature**, que basicamente diz que: quando o banco vetorial retornar uma resposta do type=N2, o atendimento deve ser escalado para um atendente humano, enviando o handoverToHumanNeeded: true na resposta.

Acredito que por se tratar de uma POC est√° seja a solu√ß√£o mais vi√°vel, por conta da feature de esclarecimento n√£o estar muito clara quais devem ser os par√¢metros utilizados, e ser muito mais regras de neg√≥cio.

### Banco de Dados

![diagrama ER](./documents/ER_diagram.png)

Escolhi por implementar um banco de dados relacional na minha aplica√ß√£o, acredito que numa implementa√ß√£o maior, utilizando microsservi√ßos, est√° minha API seja apenas um pequeno peda√ßo com responsabilidade limitada a gera√ß√£o de respostas a partir de uma entrada bem definida.

Por√©m, como gostaria de mostrar mais o que sei, decide fazer um ecosistema maior, com mais responsabilidades, como a ger√™ncia de projetos conforme o project.name, e com isso ter o controle das diferentes mensagens, conversas e tamb√©m guardar os contextos resgatadas da Azure.

Com este esquema de banco de dados √© poss√≠vel cadastrar mais projetos, conforme o nome e ‚Äòslogan‚Äô do cliente, mudando assim onde √© buscado na Azure e a quem a IA responde como assistente.

Tamb√©m √© poss√≠vel ter o hist√≥rico completo de conversar, e mensagens trocadas entre o usu√°rio e o assistente(IA), podendo assim retornar ao cliente no front de forma simples.

Com o hist√≥rico de mensagens trocadas e os contextos resgatados salvos, podemos assim se for preciso, antes mesmo de receber a pergunta do usu√°rio, podemos contextualizar a IA para assim receber a resposta mais precisa, tamb√©m pode ser projetado para d√≠gamos, sempre contextualizar a IA com as √∫ltimas perguntas feitas num intervalo de 15 minutos, assim utilizando na resposta um contexto mais amplo.

Seguindo mais no √¢mbito do chatbox, usando as perguntas feitas pelo usu√°rio, os contextos resgatados, e as respostas dada pelo assistente(IA), conseguimos avaliar a assertividade do modelo, utilizando IAs treinadas para avaliar as respostas, e assim dar nota para cada resposta/atendimento.


## Fluxo do Chatbot

- Recebe a pergunta do usu√°rio
- Gera embeddings da pergunta
- Busca trechos relevantes no banco vetorial
- Envia pergunta + contexto para o LLM gerar a resposta


## üìå Observa√ß√µes

- Certifique-se de que o √≠ndice no Azure Search j√° est√° criado e populado antes de iniciar a aplica√ß√£o.
- A aplica√ß√£o est√° configurada para ser facilmente executada via Docker, mas tamb√©m pode rodar localmente com Maven.